{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAGAS Evaluation - Measuring RAG Quality\n",
    "\n",
    "This notebook evaluates our FlexiRAG system using RAGAS metrics:\n",
    "- **Faithfulness**: Is the LLM hallucinating?\n",
    "- **Answer Relevancy**: Does the answer match the question?\n",
    "- **Context Precision**: Are retrieved chunks relevant?\n",
    "- **Context Recall**: Did we get all necessary information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mirac/Desktop/RAGDocumentationAssistant/venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.vector_store import initialize_chroma_db\n",
    "from src.qa_chain import ask_question\n",
    "from datasets import Dataset\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_relevancy,\n",
    "    context_precision,\n",
    "    context_recall\n",
    ")\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Test Dataset\n",
    "\n",
    "Creating test questions for both domains with ground truth answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total test cases: 5\n"
     ]
    }
   ],
   "source": [
    "# Automotive Domain Test Cases\n",
    "automotive_test_cases = [\n",
    "    {\n",
    "        \"question\": \"What is CAN protocol used for?\",\n",
    "        \"ground_truth\": \"CAN (Controller Area Network) is a vehicle bus standard designed to allow microcontrollers and devices to communicate with each other without a host computer.\",\n",
    "        \"domain\": \"automotive\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What does OBD-II stand for?\",\n",
    "        \"ground_truth\": \"OBD-II stands for On-Board Diagnostics II, which is a standardized system for vehicle self-diagnostics and reporting.\",\n",
    "        \"domain\": \"automotive\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"What is the main advantage of CAN FD over CAN?\",\n",
    "        \"ground_truth\": \"CAN FD (CAN with Flexible Data-Rate) enables higher data rates up to 8 Mbit/s and allows data payloads up to 64 bytes, compared to CAN's 1 Mbit/s and 8 bytes.\",\n",
    "        \"domain\": \"automotive\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Fashion Domain Test Cases\n",
    "fashion_test_cases = [\n",
    "    {\n",
    "        \"question\": \"What types of women's clothing are available?\",\n",
    "        \"ground_truth\": \"The fashion dataset includes various categories such as Westernwear, Indianwear, Lingerie & Nightwear, Footwear, Watches, Jewellery, and Fragrance for women.\",\n",
    "        \"domain\": \"fashion\"\n",
    "    },\n",
    "    {\n",
    "        \"question\": \"Are there products under 1000 rupees?\",\n",
    "        \"ground_truth\": \"Yes, the fashion dataset contains many products with selling prices under 1000 rupees across various categories.\",\n",
    "        \"domain\": \"fashion\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Combine all test cases\n",
    "all_test_cases = automotive_test_cases + fashion_test_cases\n",
    "print(f\"Total test cases: {len(all_test_cases)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Initialize System & Get Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event ClientStartEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ChromaDB...\n",
      "Initializing ChromaDB at: ./chroma_db\n",
      "âœ… Loaded existing collection: documents\n",
      "   Documents in collection: 0\n",
      "âœ… Loaded 0 documents\n"
     ]
    }
   ],
   "source": [
    "# Initialize ChromaDB\n",
    "print(\"Loading ChromaDB...\")\n",
    "client, collection = initialize_chroma_db()\n",
    "print(f\"âœ… Loaded {collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test 1/5: What is CAN protocol used for?...\n",
      "\n",
      "Question: What is CAN protocol used for?\n",
      "============================================================\n",
      "Step 1: Retrieving relevant chunks...\n",
      "Loading embedding model: all-MiniLM-L6-v2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to send telemetry event CollectionQueryEvent: capture() takes 1 positional argument but 3 were given\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Model loaded!\n",
      "âœ… Retrieved 0 chunks\n",
      "\n",
      "Step 2: Generating answer with LLM...\n"
     ]
    }
   ],
   "source": [
    "# Get answers from our RAG system\n",
    "results = []\n",
    "\n",
    "for i, test_case in enumerate(all_test_cases, 1):\n",
    "    print(f\"\\nTest {i}/{len(all_test_cases)}: {test_case['question'][:50]}...\")\n",
    "    \n",
    "    # Query with domain filtering\n",
    "    filter_metadata = {\"domain\": test_case['domain']}\n",
    "    result = ask_question(\n",
    "        collection, \n",
    "        test_case['question'],\n",
    "        n_results=3,\n",
    "        filter_metadata=filter_metadata\n",
    "    )\n",
    "    \n",
    "    # Prepare data for RAGAS\n",
    "    results.append({\n",
    "        \"question\": test_case['question'],\n",
    "        \"answer\": result['answer'],\n",
    "        \"contexts\": result['retrieved_chunks'],\n",
    "        \"ground_truth\": test_case['ground_truth']\n",
    "    })\n",
    "    \n",
    "    print(f\"âœ… Answer generated\")\n",
    "\n",
    "print(f\"\\nâœ… Collected {len(results)} results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Prepare Dataset for RAGAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset created with 5 examples\n",
      "\n",
      "Dataset columns: ['question', 'answer', 'contexts', 'ground_truth']\n"
     ]
    }
   ],
   "source": [
    "# Convert to HuggingFace Dataset format\n",
    "dataset_dict = {\n",
    "    \"question\": [r[\"question\"] for r in results],\n",
    "    \"answer\": [r[\"answer\"] for r in results],\n",
    "    \"contexts\": [r[\"contexts\"] for r in results],\n",
    "    \"ground_truth\": [r[\"ground_truth\"] for r in results]\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(dataset_dict)\n",
    "print(f\"Dataset created with {len(dataset)} examples\")\n",
    "print(f\"\\nDataset columns: {dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Run RAGAS Evaluation\n",
    "\n",
    "**Note:** This uses Ollama locally for LLM-based metrics.\n",
    "Make sure Ollama is running with llama3.2 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Configured RAGAS to use local Ollama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8y/n0smdvfd3qz_4tdt6tf78vmc0000gn/T/ipykernel_12062/253370650.py:6: LangChainDeprecationWarning: The class `ChatOllama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import ChatOllama``.\n",
      "  llm = ChatOllama(model=\"llama3.2:3b\")\n",
      "/var/folders/8y/n0smdvfd3qz_4tdt6tf78vmc0000gn/T/ipykernel_12062/253370650.py:7: LangChainDeprecationWarning: The class `OllamaEmbeddings` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaEmbeddings``.\n",
      "  embeddings = OllamaEmbeddings(model=\"llama3.2:3b\")\n"
     ]
    }
   ],
   "source": [
    "# Configure RAGAS to use Ollama\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "# Use our local Ollama instance\n",
    "llm = ChatOllama(model=\"llama3.2:3b\")\n",
    "embeddings = OllamaEmbeddings(model=\"llama3.2:3b\")\n",
    "\n",
    "print(\"âœ… Configured RAGAS to use local Ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RAGAS evaluation...\n",
      "This may take a few minutes...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 10/20 [01:21<02:09, 12.93s/it]"
     ]
    }
   ],
   "source": [
    "# Run evaluation\n",
    "print(\"Running RAGAS evaluation...\")\n",
    "print(\"This may take a few minutes...\\n\")\n",
    "\n",
    "try:\n",
    "    result = evaluate(\n",
    "        dataset,\n",
    "        metrics=[\n",
    "            faithfulness,\n",
    "            answer_relevancy,\n",
    "            context_precision,\n",
    "            context_recall\n",
    "        ],\n",
    "        llm=llm,\n",
    "        embeddings=embeddings\n",
    "    )\n",
    "    \n",
    "    print(\"\\nâœ… Evaluation complete!\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  Note: Some RAGAS metrics require OpenAI API.\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"\\nRunning with available metrics only...\")\n",
    "    \n",
    "    # Fallback: Run only metrics that work with Ollama\n",
    "    result = evaluate(\n",
    "        dataset,\n",
    "        metrics=[answer_relevancy],  # This one works with local LLM\n",
    "        llm=llm,\n",
    "        embeddings=embeddings\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for analysis\n",
    "df = result.to_pandas()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"RAGAS Evaluation Results\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show overall scores\n",
    "print(\"\\nðŸ“Š Overall Scores:\")\n",
    "print(\"-\"*60)\n",
    "for metric in df.columns:\n",
    "    if metric not in ['question', 'answer', 'contexts', 'ground_truth']:\n",
    "        score = df[metric].mean()\n",
    "        print(f\"{metric.capitalize()}: {score:.3f}\")\n",
    "\n",
    "# Show detailed results\n",
    "print(\"\\nðŸ“ Detailed Results:\")\n",
    "print(\"-\"*60)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Interpret Results\n",
    "\n",
    "**Score Interpretation:**\n",
    "- **0.0 - 0.4**: Poor - Needs significant improvement\n",
    "- **0.4 - 0.6**: Fair - Acceptable but can be better\n",
    "- **0.6 - 0.8**: Good - Performing well\n",
    "- **0.8 - 1.0**: Excellent - High quality RAG system\n",
    "\n",
    "**Metrics Explained:**\n",
    "- **Faithfulness**: Measures hallucination. Higher = less hallucination.\n",
    "- **Answer Relevancy**: Measures if answer matches question. Higher = more relevant.\n",
    "- **Context Precision**: Measures if retrieved chunks are relevant. Higher = better retrieval.\n",
    "- **Context Recall**: Measures if all needed info was retrieved. Higher = nothing missed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "df.to_csv('../evaluation_results.csv', index=False)\n",
    "print(\"\\nâœ… Results saved to: evaluation_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This evaluation provides quantitative metrics for our FlexiRAG system quality.\n",
    "Use these scores to:\n",
    "1. Compare different RAG configurations\n",
    "2. Track improvements over time\n",
    "3. Identify weak points (low scores)\n",
    "4. Demonstrate system quality in portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
