# FlexiRAG - Dynamic Multi-Domain RAG Framework

![Tests](https://github.com/miraccanince/FlexiRAG/actions/workflows/tests.yml/badge.svg)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Tests: 87 passed](https://img.shields.io/badge/tests-87%20passed-brightgreen.svg)](tests/)

A flexible, production-ready RAG (Retrieval-Augmented Generation) framework that automatically adapts to any domain. Perfect for companies wanting to deploy RAG systems with their own documents - zero code changes required.

**GitHub:** https://github.com/miraccanince/FlexiRAG

## Project Summary

**What is this?** A complete production-ready RAG (Retrieval-Augmented Generation) system with ML-powered semantic caching, JWT authentication, and modern web UI.

**Key Technical Highlights:**
- **Full-Stack ML/AI:** FastAPI backend + Streamlit frontend + LLM integration
- **Advanced ML:** Semantic cache using embedding similarity (95%+ accuracy)
- **Production-Ready:** JWT auth, rate limiting, CI/CD, 87 unit tests
- **Architecture:** ChromaDB vector store, Hybrid search (BM25 + semantic), LLM reranking

**How to Run Locally (5 minutes):**

```bash
# 1. Clone and setup
git clone https://github.com/miraccanince/FlexiRAG.git
cd FlexiRAG
python3 -m venv venv && source venv/bin/activate
pip install -r requirements.txt

# 2. Install Ollama (one-time)
brew install ollama  # macOS
ollama pull llama3.2:3b

# 3. Index sample data (already included)
python index_documents.py

# 4. Start services (2 terminals)
# Terminal 1:
uvicorn api.main:app --reload --port 8000

# Terminal 2:
streamlit run frontend/app.py
```

**Then:** Open http://localhost:8501, create an account, and start chatting!

**Tech Stack:** Python, FastAPI, Streamlit, ChromaDB, Sentence-Transformers, Ollama, JWT, Pytest

---

## What Makes This Different?

**True Framework Design:**
- Auto-discovers domains from your `data/` folder structure
- Zero code changes when adding new domains or documents
- Works with ANY company's data - just drop files in folders
- Production-ready with smart indexing and domain filtering
- 100% free and local - no API costs

**Current Status:** Production-ready with CI/CD, automated testing, user feedback, authentication, and intelligent caching

## ðŸŽ¯ Key Features

### Core Framework
- Dynamic multi-domain architecture
- Automatic domain detection from folder structure
- Multi-format support (PDF, CSV)
- Smart indexing with change detection
- Domain filtering for precise queries
- Interactive CLI with commands (/switch, /stats, /help)

### ML/AI Stack
- Local embeddings (sentence-transformers: all-MiniLM-L6-v2)
- Vector database (ChromaDB) with persistent storage
- Local LLM (Ollama + Llama 3.2 3B)
- Hybrid search (Semantic + BM25)
- LLM-based query reranking
- **Semantic Cache** - Embedding-based intelligent caching (95%+ similarity detection)
- 100% free and local (zero API costs)

### Production Features
- FastAPI backend with streaming responses
- Modern Streamlit web UI with login/register
- **JWT Authentication** - Secure user management with bcrypt password hashing
- **Rate Limiting** - IP-based API protection (10-30 req/min per endpoint)
- **Export Functionality** - CSV/JSON feedback export with date filtering
- User feedback system (thumbs up/down with analytics)
- Performance optimization & caching (360x speedup)
- CI/CD pipeline with GitHub Actions
- 87 unit tests (100% pass rate, 90%+ coverage on production features)
- Docker deployment
- Comprehensive documentation

### Current Dataset
- Automotive: 635 chunks (CAN, OBD-II, Infotainment PDFs)
- Fashion: 30,758 products (E-commerce CSV)
- Total: 31,393 indexed documents

## Tech Stack

**Backend:**
- Python 3.11+
- FastAPI (REST API)
- LangChain (RAG orchestration)
- ChromaDB (vector store)
- Sentence Transformers (all-MiniLM-L6-v2)
- Ollama + Llama 3.2 3B
- BM25 (keyword search)

**Frontend:**
- Streamlit (web UI)
- Plotly (interactive charts)

**Testing & CI/CD:**
- Pytest (37 unit tests, 100% pass rate)
- pytest-cov (81% coverage on core modules)
- GitHub Actions (automated testing on every push/PR)

**Deployment:**
- Docker & Docker Compose
- Uvicorn (ASGI server)

## Quick Start

### Prerequisites

- Python 3.11+
- [Ollama](https://ollama.ai/) (for local LLM)

### Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/miraccanince/FlexiRAG.git
   cd FlexiRAG
   ```

2. **Create virtual environment:**
   ```bash
   python3 -m venv venv
   source venv/bin/activate  # macOS/Linux
   # or
   venv\Scripts\activate  # Windows
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

4. **Install Ollama and download model:**
   ```bash
   # macOS
   brew install ollama

   # Linux
   curl -fsSL https://ollama.ai/install.sh | sh

   # Download model
   ollama pull llama3.2:3b
   ```

5. **Add your documents:**
   ```bash
   # Create domain folders and add your documents
   mkdir -p data/your_domain
   cp your_files.pdf data/your_domain/
   ```

6. **Index documents:**
   ```bash
   python index_documents.py
   ```

## Usage

### Web Interface (Recommended)

Start API and frontend:

**Terminal 1 - Start API:**
```bash
uvicorn api.main:app --reload --port 8000
```

**Terminal 2 - Start Frontend:**
```bash
streamlit run frontend/app.py
```

Then open:
- **Web UI:** http://localhost:8501
- **API Docs:** http://localhost:8000/docs

### Command Line Interface

```bash
python main.py
```

Available commands:
```
/switch   # Change domain
/stats    # View statistics
/help     # Show help
/quit     # Exit
```

### Docker Deployment

```bash
docker-compose up -d
```

Access services:
- Frontend: http://localhost:8501
- API: http://localhost:8000

## Running Tests

```bash
# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=src --cov-report=term --cov-report=html

# Open coverage report
open htmlcov/index.html
```

## Project Structure

```
FlexiRAG/
â”œâ”€â”€ src/                          # Core source code
â”‚   â”œâ”€â”€ pdf_loader.py            # PDF loading and chunking
â”‚   â”œâ”€â”€ csv_loader.py            # CSV data processing
â”‚   â”œâ”€â”€ embeddings.py            # Embedding generation
â”‚   â”œâ”€â”€ vector_store.py          # ChromaDB operations
â”‚   â”œâ”€â”€ qa_chain.py              # RAG pipeline
â”‚   â”œâ”€â”€ hybrid_search.py         # Hybrid search (Semantic + BM25)
â”‚   â”œâ”€â”€ reranker.py              # LLM-based reranking
â”‚   â”œâ”€â”€ cache_manager.py         # Query caching & performance
â”‚   â””â”€â”€ feedback_manager.py      # User feedback system
â”œâ”€â”€ api/                          # FastAPI backend
â”‚   â””â”€â”€ main.py                  # REST API endpoints
â”œâ”€â”€ frontend/                     # Streamlit frontend
â”‚   â””â”€â”€ app.py                   # Web UI
â”œâ”€â”€ tests/                        # Unit tests (37 tests)
â”‚   â”œâ”€â”€ test_embeddings.py       # Embedding tests
â”‚   â”œâ”€â”€ test_vector_store.py     # Vector store tests
â”‚   â”œâ”€â”€ test_cache_manager.py    # Cache tests
â”‚   â”œâ”€â”€ test_csv_loader.py       # CSV loader tests
â”‚   â””â”€â”€ test_feedback_manager.py # Feedback tests
â”œâ”€â”€ .github/workflows/            # CI/CD
â”‚   â””â”€â”€ tests.yml                # GitHub Actions workflow
â”œâ”€â”€ data/                         # Document storage
â”‚   â”œâ”€â”€ automotive/              # Domain 1
â”‚   â””â”€â”€ fashion/                 # Domain 2
â”œâ”€â”€ feedback/                     # User feedback data
â”‚   â””â”€â”€ feedback_store.json      # Feedback storage
â”œâ”€â”€ chroma_db/                    # Vector database (auto-generated)
â”œâ”€â”€ main.py                       # CLI interface
â”œâ”€â”€ index_documents.py            # Document indexing script
â”œâ”€â”€ requirements.txt              # Python dependencies
â””â”€â”€ docker-compose.yml            # Docker configuration
```

## API Endpoints

**Query & Search:**
- `POST /query` - Question answering with LLM (streaming)
- `POST /search` - Document search without LLM
- `GET /domains` - List available domains
- `GET /health` - System health check

**Feedback:**
- `POST /feedback` - Submit user feedback
- `GET /feedback/stats` - Get feedback analytics

**Management:**
- `POST /upload` - Upload new documents
- `DELETE /domain/{name}` - Delete domain

Full API documentation: http://localhost:8000/docs

## Example Queries

**Automotive:**
- "What is CAN protocol used for?"
- "How does OBD-II diagnostic work?"
- "Explain infotainment system architecture"

**Fashion (E-commerce):**
- "Show me dresses under 1000 rupees"
- "What brands sell western wear?"
- "Find jewellery products"

## Documentation

- [FEEDBACK_SYSTEM.md](FEEDBACK_SYSTEM.md) - User feedback system implementation
- [CI_CD_GUIDE.md](CI_CD_GUIDE.md) - CI/CD pipeline guide
- [tests/README.md](tests/README.md) - Testing documentation

## Development

### Adding New Domains

1. Create folder in `data/`:
   ```bash
   mkdir data/new_domain
   ```

2. Add documents (PDF or CSV):
   ```bash
   cp documents.pdf data/new_domain/
   ```

3. Reindex:
   ```bash
   python index_documents.py
   ```

The system automatically detects and indexes the new domain.

### Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Run tests: `pytest tests/ -v`
5. Submit a pull request

CI/CD pipeline will automatically run tests on your PR.

## Performance

- Average query time: 3.6s (with caching: ~0.01s)
- Cache hit rate: ~85% in production use
- Speedup with caching: 360x
- Index size: 31,393 documents
- Coverage: 90%+ on production features (auth, semantic cache, vector store, feedback)
- Semantic cache similarity: 95%+ for paraphrased queries

## Technical Highlights for Interviews

### Advanced ML/AI Implementation

**1. Semantic Cache (Novel Approach)**
- Problem: Traditional string-based caching misses paraphrased queries
- Solution: Embedding-based cache using cosine similarity (threshold: 0.85)
- Result: 95.6% similarity match for "What is OBD-II?" vs "What does OBD-II mean?"
- Impact: Instant responses for semantically similar questions

**2. Hybrid Search Architecture**
- Combines semantic search (ChromaDB) with BM25 keyword search
- LLM-based reranking for optimal results
- Achieves better accuracy than either method alone

**3. Production-Ready Security**
- JWT authentication with bcrypt password hashing
- OAuth2-compatible token system
- Rate limiting (10-30 req/min per endpoint)
- Protected endpoints for upload/delete operations

### Engineering Excellence

**4. CI/CD Pipeline**
- GitHub Actions running 87 unit tests on every commit
- 90%+ code coverage on production features (auth: 93%, semantic cache: 91%, vector store: 93%)
- Automated dependency caching
- Coverage reports as downloadable artifacts

**5. Full-Stack Development**
- Backend: FastAPI with async streaming responses
- Frontend: Streamlit with session-based auth
- Database: ChromaDB vector store with 31K+ documents
- Testing: Pytest with comprehensive test suite

**6. Scalable Architecture**
- Multi-domain support without code changes
- Incremental document indexing
- Thread-safe caching and user management
- LRU eviction policy for semantic cache

### Key Design Decisions

- **Why local LLM?** Zero API costs, data privacy, full control
- **Why Semantic Cache?** Reduces LLM calls by ~85%, instant responses
- **Why JWT?** Stateless authentication, scalable across services
- **Why Hybrid Search?** Combines strengths of semantic and keyword search

## License

MIT License - See LICENSE file for details

## Author

**Mirac Can Ince**

Portfolio project demonstrating production-ready RAG systems for ML Engineer positions.

Built using Python, ChromaDB, and Llama 3.2 | 100% Free & Local
